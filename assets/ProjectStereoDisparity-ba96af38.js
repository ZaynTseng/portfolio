import{r as o,o as i,c as s,w as a,B as r,a as e,e as l,f as u,b as n}from"./index-3751e89f.js";const d="/portfolio/assets/comparison-of-fixed-window-18ddefc4.png",h="/portfolio/assets/comparison-of-dynamic-window-b82b7bd1.png",p={class:"max-w-3xl gap-2 grid"},c=e("figure",null,[e("img",{src:u,alt:"Stereo Disparity",class:"border-base-content bg-base-300 rounded-box border border-opacity-5"})],-1),m=e("h1",{class:"text-5xl font-bold pt-4"},"Stereo Disparity",-1),f=e("p",{class:"text-2xl"},"Stereo disparity algorithm experiments and evaluation",-1),g=e("div",{class:"gap-2 flex pb-6"},[e("span",{class:"badge badge-primary"},"Algorithms"),e("span",{class:"badge badge-secondary"},"Computer Vision"),e("span",{class:"badge badge-ghost"},"October 2022")],-1),w=e("h2",{class:"text-3xl font-bold pt-4"},"Introduction",-1),b=e("p",null,"Stereo matching, which estimates an accurate depth from stereo pair images, is a critical technology for helping achieve high-performance 3D applications in the field of computer vision [1]. Many different methods and pre-processing steps have been proposed in this field. In this paper, to calculate the disparity maps of the given stereo image pairs, different stereo matching algorithms will be compared and the possible optimizations from different aspects will be discussed.",-1),_=e("p",null,[n("The dataset in this experiment is curated from "),e("a",{class:"link link-hover",target:"_blank",href:"https://drivingstereo-dataset.github.io"},"here ↗"),n(". Totally, there are 25 sets of images, each set has a pair of stereo images taken from a moving vehicle and one ground truth depth image. Thus, the disparity map we calculated should be compared with the true depth image to check correctness.")],-1),y=e("p",null,"As for the structure of this paper, methods used in the experiment and justification will first be given. Then, the specific process of all experiments and the evaluation will be addressed. Lastly, the possible improvement of future research will be discussed.",-1),v=e("h2",{class:"text-3xl font-bold pt-4"},"Literature Review",-1),x=e("h3",{class:"text-2xl font-bold pt-4"},"Stereo Disparity",-1),S=e("p",null,"As a way to mimic how the human eyes perceive an image from different positions and process such differences to reveal the scene’s depth information [2], computer vision also adopted a similar mechanism to find cues for depth. In computer vision, stereo matching is a task to estimate the depth information from a pair of images that are normally taken by two horizontally aligned cameras. In detail, it requires the algorithm to find matching features from the right image to the left image to get the disparity of pixels and resulting in a disparity map, where the disparity is inversely proportional to the distance from the observer [3]. This ability to understand the image in the z-coordinate provides the foundation for many applications such as object detection, 3D reconstructs, navigation, and autopilot vehicles.",-1),C=e("p",null,"To find corresponding features in the image pair, a local approach is sliding window, which will traverse the image pair and use a cost function to evaluate the pixels in that window and find the best matching blocks for each pixel in the left image from the right image pair, thus calculating the disparity. A traditional approach of sliding window is sum of squared-differences (SSD), a later one is sum of absolute differences (SAD), and normalised cross-correlation (NCC). However, as the name suggests, local approaches inherently lacking consideration of global relations, thus, their results normally have lots of artefacts and noise. To solve this issue, global algorithms have smoothness assumptions and tried to optimise them with a global cost function (likely an energy minimisation formula). Common global algorithms include probabilistic diffusion [4], and graph cuts [5]. In this study, the local approach is taken, as they are easier to implement and evaluate.",-1),D=e("h2",{class:"text-3xl font-bold pt-4"},"Methods and Justification",-1),N=e("h3",{class:"text-2xl font-bold pt-4"},"A. SSD",-1),k=e("p",null,"Sum of squared-differences (SSD) is a matching cost computation metric of the pixel values in two windows, where both windows should be in the same shape. For SSD, the smallest value in a search range would be the best match. SSD serves as a baseline in this study.",-1),M=e("h3",{class:"text-2xl font-bold pt-4"},"B. NCC",-1),z=e("p",null,"Different from SSD, normalised cross-correlation (NCC) takes into consideration of the gain differences due to normalisation, therefore, it would be more tolerant to brightness changes in the two matching windows and resulting in a better performance than SSD. The higher the NCC, the better the similarity between the two windows.",-1),T=e("h3",{class:"text-2xl font-bold pt-4"},"C. ZNCC",-1),F=e("p",null,"Zero-mean normalised cross-correlation (ZNCC) is an adaptation of NCC that compensates the constant offsets in pixel values during matching, this makes it more robust to biases in the matching windows and thus has an increased performance than NCC.",-1),I=e("h3",{class:"text-2xl font-bold pt-4"},"D. Design Choices in Implementation",-1),A=e("p",null,"Apart from the basic for loop version of the window matching algorithm that traverses the whole image row by row and pixel by pixel, which is workable but very inefficient, two variants are also implemented to improve the performance. First is the kernel computation version of the same algorithm but using cv2.filter2D to accelerate the summation operation in SSD, NCC, and ZNCC. In this kernel computation version, the summations in each for loop on windows are replaced by one single operation on the whole image of applying the cv2.filter2D function with a filter full of ones at the window size, this can lead to a much faster computation time. Another variant is trying to solve the fixed window size issue. In the basic version, the search window size is fixed for all the operations for each pixel, however, this might be problematic since one size may not suit all situations on the image (e.g., some features might be larger while some others are smaller). Therefore, in the modified version, a range of window size will be applied on the same pixel for the left image and its corresponding right pixel in the search range, and the SSD, NCC, or ZNCC metric will be calculated to find the best matching window and its disparity. This approach introduced flexibility in the window sizes and may lead to better performance. Finally, grid search is used to find the best parameter of window size and search range as well.",-1),R=e("h3",{class:"text-2xl font-bold pt-4"},"E. Evaluation Metrics",-1),B=e("p",null,"To evaluate the performance of different approaches, root mean squared error (RMS), the fraction of good matching pixels within a given error range, and computation time are used. RMS is computed between the disparity map computed dC(x, y) and the ground truth dT(x, y), i.e.",-1),Z=e("p",null,"Where N is the total number of pixels where the ground truth image has a value. For fraction of good matching pixels, the range of errors is 4, 2, 1, 0.5 and 0.25 pixels between the disparity map computed and the ground truth image. The computation time is compared in seconds.",-1),E=e("h2",{class:"text-3xl font-bold pt-4"},"Experiments and Evaluation",-1),V=e("p",null,"During the experiment of this task, the SSD algorithm has been chosen as the baseline and other different algorithms of stereo matching have been used and compared. Additionally, to optimize the performance of the results, we also consider 5 aspects: the size of the window in the algorithm, the impact of different pixels, the sub-pixel accuracy, the smooth output and the acceleration. We do these five optimizations separately on the basic SSD algorithm and compare their results to see if they were valid. As for the comparison method, we use the average evaluation indicators (the RMS error, the fractions and the runtime) for the whole data set.",-1),L=e("h3",{class:"text-2xl font-bold pt-4"},"A. SSD vs. NCC vs. ZNCC",-1),H=e("p",null,"Firstly, we separately use SSD, NCC and ZNCC algorithms, which the paper explains previously. If the window size is set as 31, the result will be:",-1),j=e("div",{class:"overflow-x-auto"},[e("table",{class:"table"},[e("thead",null,[e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"Method")]),e("td",{colspan:"3"},[e("p",null,"Different Metrics")])]),e("tr",null,[e("td",null,[e("p",null,"RMS")]),e("td",null,[e("p",null,"Fraction")]),e("td",null,[e("p",null,"Time (25 images)")])])]),e("tbody",null,[e("tr",null,[e("td",null,[e("p",null,"SSD")]),e("td",null,[e("p",null,"16.12")]),e("td",null,[e("p",null,"[69.49, 53.76, 38.24, 23.33, 12.08]")]),e("td",null,[e("p",null,"8.74s")])]),e("tr",null,[e("td",null,[e("p",null,"NCC")]),e("td",null,[e("p",null,"8.84")]),e("td",null,[e("p",null,"[83.75, 62.83, 43.11, 25.86, 13.35]")]),e("td",null,[e("p",null,"12.8s")])]),e("tr",null,[e("td",null,[e("p",null,"ZNCC")]),e("td",null,[e("p",null,"8.08")]),e("td",null,[e("p",null,"[83.59, 62.05, 42.61, 25.61, 13.24]")]),e("td",null,[e("p",null,"18.1s")])])])])],-1),q=e("p",null,"Obviously, SSD has the worst performance and the ZNCC get the best performance. It meets our expectations, since theoretically, the NCC, which uses more complex formulas to reduce the linear variation in the brightness, will perform better than SSD. Additionally, since ZNCC is immune to intensity distortions, it will get more accurate results [6]. However, due to the more complex formula, the runtime of ZNCC is the longest. Overall, the RMS error still can be lower, since all these three algorithms are local matching algorithms, which may lose the information of the surrounding pixels.",-1),W=e("h3",{class:"text-2xl font-bold pt-4"},"B. Window Size ",-1),G=e("p",null,"Considering different window sizes will influence the result, firstly, we use the grid search to find the best size. The size is set between 10 and 40, and part of the results are:",-1),J=e("div",{class:"overflow-x-auto"},[e("table",{class:"table"},[e("thead",null,[e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"Metric")]),e("td",{colspan:"6"},[e("p",null,"Different disparity range")])]),e("tr",null,[e("td",null,[e("p",null,"25")]),e("td",null,[e("p",null,"27")]),e("td",null,[e("p",null,"29")]),e("td",null,[e("p",null,"31")]),e("td",null,[e("p",null,"33")]),e("td",null,[e("p",null,"35")])])]),e("tbody",null,[e("tr",null,[e("td",null,[e("p",null,"RMS")]),e("td",null,[e("p",null,"16.51")]),e("td",null,[e("p",null,"16.35")]),e("td",null,[e("p",null,"16.20")]),e("td",null,[e("p",null,"16.07")]),e("td",null,[e("p",null,"15.94")]),e("td",null,[e("p",null,"15.80")])]),e("tr",null,[e("td",null,[e("p",null,"Fraction")]),e("td",null,[e("p",null,"[69.84, 56.62, 40.88, 25.21, 13.13]")]),e("td",null,[e("p",null,"[69.98, 55.70, 39.99, 24.58, 12.77]")]),e("td",null,[e("p",null,"[69.85, 54.74, 39.11, 24.00, 12.44]")]),e("td",null,[e("p",null,"[69.53, 53.74, 38.23, 23.33, 12.07]")]),e("td",null,[e("p",null,"[69.02, 52.77, 37.36, 22.72, 11.75]")]),e("td",null,[e("p",null,"[68.37, 51.81, 36.53, 22.14, 11.42]")])])])])],-1),P=e("p",null,"Combining the result of RMS and Fraction, we set 31 as the window size. Furthermore, considering that pixels in different parts of the image may be affected differently by surrounding pixels, we also try to use the dynamic window size, the result is:",-1),O=e("div",{class:"overflow-x-auto"},[e("table",{class:"table"},[e("thead",null,[e("tr",null,[e("td",{colspan:"2",rowspan:"2"},[e("p",null,"Method")]),e("td",{colspan:"3"},[e("p",null,"Different metrics")])]),e("tr",null,[e("td",null,[e("p",null,"RMS")]),e("td",null,[e("p",null,"Fraction")]),e("td",null,[e("p",null,"Time (25)")])])]),e("tbody",null,[e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"SSD")]),e("td",null,[e("p",null,"Fixed")]),e("td",null,[e("p",null,"16.12")]),e("td",null,[e("p",null,"[69.49, 53.76, 38.24, 23.33, 12.08]")]),e("td",null,[e("p",null,"8.74s")])]),e("tr",null,[e("td",null,[e("p",null,"Dynamic")]),e("td",null,[e("p",null,"15.74")]),e("td",null,[e("p",null,"[69.35, 53.70, 38.32,23.39, 12.09]")]),e("td",null,[e("p",null,"83s")])]),e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"NCC")]),e("td",null,[e("p",null,"Fixed")]),e("td",null,[e("p",null,"8.84")]),e("td",null,[e("p",null,"[83.75, 62.83, 43.11, 25.86, 13.35]")]),e("td",null,[e("p",null,"12.8s")])]),e("tr",null,[e("td",null,[e("p",null,"Dynamic")]),e("td",null,[e("p",null,"8.47")]),e("td",null,[e("p",null,"[83.85, 64.00, 43.34, 26.74, 13.80]")]),e("td",null,[e("p",null,"137s")])]),e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"ZNCC")]),e("td",null,[e("p",null,"Fixed")]),e("td",null,[e("p",null,"8.08")]),e("td",null,[e("p",null,"[83.59, 62.05, 42.61, 25.61, 13.24]")]),e("td",null,[e("p",null,"18.1s")])]),e("tr",null,[e("td",null,[e("p",null,"Dynamic")]),e("td",null,[e("p",null,"7.72")]),e("td",null,[e("p",null,"[85.51, 65.23, 45.20, 27.30, 14.11]")]),e("td",null,[e("p",null,"192s")])])])])],-1),K=e("p",null,"Here are the disparity maps we got from different methods and the ground truth depth image given in the dataset:",-1),Y=e("img",{src:d,alt:"comparison-of-fixed-window"},null,-1),Q=e("span",null,"Fig. 1. Comparison of fixed window",-1),U=e("img",{src:h,alt:"comparison-of-dynamic-window"},null,-1),X=e("span",null,"Fig. 2. Comparison of dynamic window",-1),$=e("p",null,"The method of dynamic window size performs better, which corroborates our thoughts, while the drawback of this method is that it costs more time.",-1),ee=e("h3",{class:"text-2xl font-bold pt-4"},"C. Gaussian and Sharpen ",-1),te=e("p",null,"To explore the impact of different pixels, at first, we assume the centre of the window has a more important effect, so we add Gaussian blur to the window. Moreover, since the value is calculated by the brightness of the grey image, we think that sharpening the image to enhance grey-scale jumping may be useful. The result can be seen below:",-1),le=e("div",{class:"overflow-x-auto"},[e("table",{class:"table"},[e("thead",null,[e("tr",null,[e("td",{colspan:"2",rowspan:"2"},[e("p",null,"Method")]),e("td",{colspan:"2"},[e("p",null,"Different metrics")])]),e("tr",null,[e("td",null,[e("p",null,"RMS")]),e("td",null,[e("p",null,"Fraction")])])]),e("tbody",null,[e("tr",null,[e("td",{rowspan:"3"},[e("p",null,"SSD")]),e("td",null,[e("p",null,"Base")]),e("td",null,[e("p",null,"16.12")]),e("td",null,[e("p",null,"[69.49, 53.76, 38.24, 23.33, 12.08]")])]),e("tr",null,[e("td",null,[e("p",null,"Gaussian")]),e("td",null,[e("p",null,"17.4")]),e("td",null,[e("p",null,"[64.15, 57.22, 44.27, 27.99, 14.72]")])]),e("tr",null,[e("td",null,[e("p",null,"Sharpen")]),e("td",null,[e("p",null,"15.58")]),e("td",null,[e("p",null,"[67.65, 50.49, 36.48, 23.11, 12.12]")])]),e("tr",null,[e("td",{rowspan:"3"},[e("p",null,"NCC")]),e("td",null,[e("p",null,"Base")]),e("td",null,[e("p",null,"8.84")]),e("td",null,[e("p",null,"[83.75, 62.83, 43.11, 25.86, 13.35]")])]),e("tr",null,[e("td",null,[e("p",null,"Gaussian")]),e("td",null,[e("p",null,"12.41")]),e("td",null,[e("p",null,"[77.17, 68.44, 51.89, 32.63, 17.19]")])]),e("tr",null,[e("td",null,[e("p",null,"Sharpen")]),e("td",null,[e("p",null,"15.1")]),e("td",null,[e("p",null,"[68.21, 50.69, 36.58, 23.16, 12.14]")])]),e("tr",null,[e("td",{rowspan:"3"},[e("p",null,"ZNCC")]),e("td",null,[e("p",null,"Base")]),e("td",null,[e("p",null,"8.08")]),e("td",null,[e("p",null,"[83.59, 62.05, 42.61, 25.61, 13.24]")])]),e("tr",null,[e("td",null,[e("p",null,"Gaussian")]),e("td",null,[e("p",null,"12.41")]),e("td",null,[e("p",null,"[77.16, 68.43, 51.88, 32.62, 17.19]")])]),e("tr",null,[e("td",null,[e("p",null,"Sharpen")]),e("td",null,[e("p",null,"10.54")]),e("td",null,[e("p",null,"[74.97, 54.53, 38.66, 24.26, 12.7]")])])])])],-1),ne=e("p",null,"It can be seen that both of these two methods fail to improve the performance on both RMS and Fractions, which means the centre pixel may have more complicated relations to its neighbours, such as the colour difference or the shading relationships.",-1),oe=e("h3",{class:"text-2xl font-bold pt-4"},"D. Sub-pixel Accuracy ",-1),ie=e("p",null,"Since we found the disparity in the ground truth is not integers, if we do not do any processing, the disparity we get from stereo matching algorithms can only be an integer, which will lose accuracy. We try to enlarge the image, then calculate the disparity and scale the disparity map back to the image's original size. Lastly, we divide the disparity by the zoom ratio, so that the sub-pixel can be gotten. Here are the results:",-1),se=e("div",{class:"overflow-x-auto"},[e("table",{class:"table"},[e("thead",null,[e("tr",null,[e("td",{colspan:"2",rowspan:"2"},[e("p",null,"Method")]),e("td",{colspan:"2"},[e("p",null,"Different metrics")])]),e("tr",null,[e("td",null,[e("p",null,"RMS")]),e("td",null,[e("p",null,"Fraction")])])]),e("tbody",null,[e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"SSD")]),e("td",null,[e("p",null,"Base")]),e("td",null,[e("p",null,"16.12")]),e("td",null,[e("p",null,"[69.49, 53.76, 38.24, 23.33, 12.08]")])]),e("tr",null,[e("td",null,[e("p",null,"Resize")]),e("td",null,[e("p",null,"17.27")]),e("td",null,[e("p",null,"[65.87, 57.97, 44.53, 29.54, 16.64]")])]),e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"NCC")]),e("td",null,[e("p",null,"Base")]),e("td",null,[e("p",null,"8.84")]),e("td",null,[e("p",null,"[83.75, 62.83, 43.11, 25.86, 13.35]")])]),e("tr",null,[e("td",null,[e("p",null,"Resize")]),e("td",null,[e("p",null,"11.28")]),e("td",null,[e("p",null,"[79.95, 69.48, 51.62, 33.67, 19.00]")])]),e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"ZNCC")]),e("td",null,[e("p",null,"Base")]),e("td",null,[e("p",null,"8.08")]),e("td",null,[e("p",null,"[83.59, 62.05, 42.61, 25.61, 13.24]")])]),e("tr",null,[e("td",null,[e("p",null,"Resize")]),e("td",null,[e("p",null,"10.93")]),e("td",null,[e("p",null,"[81.12, 70.06, 51.89, 33.86, 19.06]")])])])])],-1),ae=e("p",null,"Although the RMS get worse, the fractions of pixels with errors less than 2, 1, 0.5 and 0.25 pixels are all improved, which means the accuracy is improved. However, the number of pixels with errors less than 4 gets larger, so there will be other better and more reasonable ways to get the sub-pixel.",-1),re=e("h3",{class:"text-2xl font-bold pt-4"},"E. Smooth",-1),ue=e("p",null,"Inspired by the formula:",-1),de=e("p",null,"We implement it with a simple method: Calculate the disparity map first based on the SSD algorithm, and then use this formula to smooth the result. Since this method requires two complete operations, to save the runtime, we design the kernel for it. Lastly, we use grid search to set the value of λ.",-1),he=e("p",null,"From TABLE IV, we can know that there is indeed an improvement, but it is very insignificant. This is due to the method we use being a simplified version, which has a fixed disparity map.",-1),pe=e("p",null,"Specifically, when calculating the smooth, we consider all 8 pixels surrounding the centre pixel. In the design of the kernel algorithm for SSD, we use convolution operations to calculate the whole image’s values of SSD for each disparity and set the max disparity value for looping. In other words, the result is a three-dimensional matrix: the x and y dimension is the horizontal and vertical coordinates of the image; the z dimension is the range of values for the disparity. Thus, to capture the disparity map is to find the minimum z for each of the coordinates. Continuing this thought, we have to add the smooth cost for each value. Since we consider the differences in a disparity between the centre and the 8 neighbours, a 3x3 kernel filled with 1 except for 0 in the centre can be designed. Then, we can use this kernel to convolve with the disparity map obtained from the first calculation to get a smooth disparity map (SDP). Then the following equation can be used to calculate the smooth cost for each disparity:",-1),ce=e("p",null,"Lastly, we use the same method as the kernel method: to find the minimum z for each of the coordinates.",-1),me=e("div",{class:"overflow-x-auto"},[e("table",{class:"table"},[e("thead",null,[e("tr",null,[e("td",{colspan:"2",rowspan:"2"},[e("p",null,"Method")]),e("td",{colspan:"2"},[e("p",null,"Different metrics")])]),e("tr",null,[e("td",null,[e("p",null,"RMS")]),e("td",null,[e("p",null,"Fraction")])])]),e("tbody",null,[e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"SSD")]),e("td",null,[e("p",null,"Base")]),e("td",null,[e("p",null,"16.12")]),e("td",null,[e("p",null,"[69.49, 53.76, 38.24, 23.33, 12.08]")])]),e("tr",null,[e("td",null,[e("p",null,"Smooth")]),e("td",null,[e("p",null,"15.97")]),e("td",null,[e("p",null,"[69.49, 53.82, 38.28, 23.35, 12.08]")])]),e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"NCC")]),e("td",null,[e("p",null,"Base")]),e("td",null,[e("p",null,"8.84")]),e("td",null,[e("p",null,"[83.75, 62.83, 43.11, 25.86, 13.35]")])]),e("tr",null,[e("td",null,[e("p",null,"Smooth")]),e("td",null,[e("p",null,"7.63")]),e("td",null,[e("p",null,"[84.05, 62.59, 42.74, 25.62, 13.23]")])]),e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"ZNCC")]),e("td",null,[e("p",null,"Base")]),e("td",null,[e("p",null,"8.08")]),e("td",null,[e("p",null,"[83.59, 62.05, 42.61, 25.61, 13.24]")])]),e("tr",null,[e("td",null,[e("p",null,"Smooth")]),e("td",null,[e("p",null,"7.63")]),e("td",null,[e("p",null,"[83.59, 62.05, 42.61, 25.61, 13.24]")])])])])],-1),fe=e("p",null,"From TABLE IV, we can know that there is indeed an improvement, but it is not significant enough. This is due to the method we use being a simplified version, which has a fixed disparity map. If we truly implement this formula, the result will be better.",-1),ge=e("h3",{class:"text-2xl font-bold pt-4"},"F. Acceleration",-1),we=e("p",null,"Due to the principle of stereo matching: for a point in the real world, the position that appears on the left image must be to the left of the position on the right image, we could set the left range in the right image based on the coordinates of one pixel on the left, then the matching pixel can be found in this range. By narrowing the range of matches, we believe it can be improved in both accuracy and time. We use grid search to find the result:",-1),be=e("div",{class:"overflow-x-auto"},[e("table",{class:"table"},[e("thead",null,[e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"Metric")]),e("td",{colspan:"6"},[e("p",null,"Different disparity range")])]),e("tr",null,[e("td",null,[e("p",null,"40")]),e("td",null,[e("p",null,"50")]),e("td",null,[e("p",null,"60")]),e("td",null,[e("p",null,"70")]),e("td",null,[e("p",null,"80")]),e("td",null,[e("p",null,"90")])])]),e("tbody",null,[e("tr",null,[e("td",null,[e("p",null,"RMS")]),e("td",null,[e("p",null,"20.12")]),e("td",null,[e("p",null,"18.31")]),e("td",null,[e("p",null,"17.19")]),e("td",null,[e("p",null,"16.83")]),e("td",null,[e("p",null,"16.92")]),e("td",null,[e("p",null,"17.04")])]),e("tr",null,[e("td",null,[e("p",null,"Fraction")]),e("td",null,[e("p",null,"[58.35, 49.50, 37.50, 23.71, 12.47]")]),e("td",null,[e("p",null,"[63.78, 53.74, 39.93, 25.01, 13.11]")]),e("td",null,[e("p",null,"[67.66, 56.78, 41.78, 26.01, 13.61]")]),e("td",null,[e("p",null,"[68.86, 58.03, 42.56, 26.41, 13.81]")]),e("td",null,[e("p",null,"[68.74, 57.98, 42.55, 26.41, 13.81]")]),e("td",null,[e("p",null,"[68.71, 57.96, 42.54, 26.40, 13.80]")])])])])],-1),_e=e("p",null,"It can be seen that the range of 70 could be the best choice.",-1),ye=e("p",null,"Moreover, we also design the kernel which the paper has discussed before to accelerate the process. Here is the result:",-1),ve=e("div",{class:"overflow-x-auto"},[e("table",{class:"table"},[e("thead",null,[e("tr",null,[e("td",{rowspan:"2"},[e("p",null,"Metric")]),e("td",{colspan:"6"},[e("p",null,"Different Methods")])]),e("tr",null,[e("td",null,[e("p",null,"SSD")]),e("td",null,[e("p",null,"SDD kernel")]),e("td",null,[e("p",null,"SDD SepFillter")]),e("td",null,[e("p",null,"NCC")]),e("td",null,[e("p",null,"NCC Kennel")]),e("td",null,[e("p",null,"NCC SepFillter")])])]),e("tbody",null,[e("tr",null,[e("td",null,[e("p",null,"Time (1 img)")]),e("td",null,[e("p",null," 95s")]),e("td",null,[e("p",null,"665ms")]),e("td",null,[e("p",null,"357ms")]),e("td",null,[e("p",null,"182s")]),e("td",null,[e("p",null,"1.52s")]),e("td",null,[e("p",null,"567ms")])])])])],-1),xe=e("p",null,"A significant reduction in time can be seen. This is because by using convolutional operations, two layers of loops can be reduced, thus greatly improving the efficiency of operations.",-1),Se=e("h2",{class:"text-3xl font-bold pt-4"},"Conclusion and Future Improvements",-1),Ce=e("p",null,"This report started by introducing the concept of stereo matching and its importance and use of it. It then introduced the main design of this report and the dataset used for the experimentation. In the next section, this report went over the detail of stereo disparity and common approaches used to solve it in computer vision, such as local and global approaches. It then described the methods used in this study, including metrics like SSD, NCC, and ZNCC, different design choices as well as evaluating approaches. It is followed by a detailed discussion of the experiments conducted, such as adjustments of window size, different pre-processing methods, and smoothing techniques. The differences in performance with possible reasons are then analysed.",-1),De=e("p",null,"To conclude, ZNCC has the best performance in both RMS and fraction of good matching pixels than SSD and NCC. Flexible window size also leads to a better performance than fixed window sizes, even if the window size is chosen from good-performing grid search results. As for the pre-processing methods, it seems like both Gaussian blur and sharpening would not contribute to a better result in this study’s scenario, no matter whether the filter is applied to the whole image or on the matching windows. However, the simplified smoothing technique does lead to a slight improvement in performance. Lastly, it is also observed that kernel operation is much faster than the basic for loop traverse implementation of the same algorithm, kernel computation can be more than 200 times faster than the traverse version.",-1),Ne=e("p",null,"For future directions, it is recommended that global approaches like graph cuts are more worthwhile to implement than local approaches, due to their better consideration of the neighbour information and can produce a smoother output. Moreover, sub-pixel accuracy can be applied in are more appropriate way. Apart from these traditional ways, machine learning aided methods such as supervised depth classification are also worth exploring.",-1),ke=e("h2",{class:"text-3xl font-bold pt-4"},"References",-1),Me=e("p",null,"[1] M. Jang, H. Yoon, S. Lee, J. Kang, and S. Lee, ‘A Comparison and Evaluation of Stereo Matching on Active Stereo Images’, Sensors, vol. 22, no. 9, p. 3332, Apr. 2022, doi: 10.3390/s22093332. ",-1),ze=e("p",null,"[2] N. Qian, ‘Binocular Disparity and the Perception of Depth’, Neuron, vol. 18, no. 3, pp. 359–368, Mar. 1997, doi: 10.1016/S0896-6273(00)81238-6. ",-1),Te=e("p",null,"[3] K. Prazdny, ‘Detection of Binocular Disparities’, in Readings in Computer Vision, Elsevier, 1987, pp. 73–79. doi: 10.1016/B978-0-08-051581-6.50014-3. ",-1),Fe=e("p",null,"[4] D. Scharstein and R. Szeliski, ‘Stereo Matching with Nonlinear Diffusion’, Int. J. Comput. Vis., vol. 28, no. 2, pp. 155–174, Jun. 1998, doi: 10.1023/A:1008015117424. ",-1),Ie=e("p",null,"[5] Y. Boykov, O. Veksler, and R. Zabih, ‘Fast approximate energy minimization via graph cuts’, IEEE Trans. Pattern Anal. Mach. Intell., vol. 23, no. 11, pp. 1222–1239, Nov. 2001, doi: 10.1109/34.969114. ",-1),Ae=e("p",null,"[6] S. Patil, J. S. Nadar, J. Gada, S. Motghare, and S. S. Nair, ‘Comparison of Various Stereo Vision Cost Aggregation Methods’, vol. 2, no. 8, p. 5, 2013.",-1),Re=e("h2",{class:"text-3xl font-bold pt-4"},"Authors",-1),Be=e("p",null,"This is a team project proudly produced by Minghua Zhang and Zhi Zeng at the University of Melbourne in October 2022.",-1),He={__name:"ProjectStereoDisparity",setup(Ze){return(Ee,Ve)=>{const t=o("katex-element");return i(),s(r,{class:"animate__animated animate__fadeIn"},{default:a(()=>[e("div",p,[c,m,f,g,w,b,_,y,v,x,S,C,D,N,k,M,z,T,F,I,A,R,B,l(t,{expression:"R={(\\frac{1}{N}\\sum_{(x,y)}{\\left|d_C\\left(x,y\\right)-\\ d_T\\left(x,y\\right)\\right|^2)}}^\\frac{1}{2}"}),Z,E,V,L,H,j,q,W,G,J,P,O,K,Y,Q,U,X,$,ee,te,le,ne,oe,ie,se,ae,re,ue,l(t,{expression:`E\\left(D\\right)=\\ \\sum_{i}\\left(W_1\\left(i\\right)-W_2\\left(i+D\\left(i\\right)\\right)\\right)^2
+\\ \\lambda\\sum_{i,j}{\\rho(D\\left(i\\right)-D(j))}`}),de,he,pe,l(t,{expression:"Smooth=\\left|8\\ast S S D_{diparity\\ map}-\\ \\ SDP\\right|\\ast cost"}),ce,me,fe,ge,we,be,_e,ye,ve,xe,Se,Ce,De,Ne,ke,Me,ze,Te,Fe,Ie,Ae,Re,Be])]),_:1})}}};export{He as default};
